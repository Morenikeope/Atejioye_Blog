[
  {
    "objectID": "posts/Report_Draft/Report_Draft.html",
    "href": "posts/Report_Draft/Report_Draft.html",
    "title": "Housing Project Report",
    "section": "",
    "text": "Crucial to Midwestern rural vitality is a supply of good quality housing for residents of all ages and income levels. Crucial to effective rural housing policy development is a supply of good quality data describing local housing conditions.\nIdeally, communities would have ready access to information about their local housing stock to identify needs, set priorities, and optimize allocation of resources and investments. Unfortunately, many small communities lack the capacity to systematically evaluate their housing stock.\nCity and county property assessors’ offices seem a logical source for obtaining local housing data; however, tapping into these assessor databases poses some challenges. Access to a community’s full database may be restricted, as most city and county assessors now outsource data management to private firms. Even when accessible, the database may not be amenable to custom tabulations and targeted queries about specific housing conditions. In addition, the subjectivity of the assessment process introduces biases and inconsistences into the data themselves.\nData science approaches including Artificial Intelligence (AI) models offer great potential for addressing these and other housing data challenges encountered by rural communities and researchers who study them. For example, image classification models could be used to rate the condition of selected exterior housing features, such as roof and siding, or to detect the presence of problems such as missing or damaged gutters or an overgrown landscape. Leveraging AI technology in this manner could streamline the housing evaluation process, eliminate subjective biases, and facilitate informed decision-making for housing investment and development initiatives in rural communities.\n\n\n\nThe goal of this project is to investigate methods for conducting a thorough and objective evaluation of a community’s housing stock using AI, and also to explore how such methods could be adapted for multi-community analysis of relationships between housing quality and other local attributes in rural areas.\nObjectives:\n\nConduct a literature review to examine examples of AI applications in planning and housing.\nIdentify study communities and collect data about the communities.\nCollect housing data, including images, from existing sources such as realtor websites, Google Street View and county assessor websites.\nCollect data on housing age, number of bedrooms, square footage, and assessed value.\nUtilize roof, gutter, siding, and landscape condition as part of AI analysis to identify homes that may qualify for community assistance programs.\nAssess the results and recommend next steps for Year 2 of the project, considering the insights gained and community input received.\n\n\n\n\nAn important objective of the project was its focus on reviewing as much accessible literature and websites. Review of how cities have been using AI to address problems, and specifically housing problems, and how counties or cities assess housing conditions. The section also answers the question, what data might we use to assess housing condition in this project?\n\nAmes City Housing Inspection\n\nIn the context of housing inspections, the City of Ames in Iowa has implemented a comprehensive housing inspection program to ensure housing quality and safety standards are met. The Ames City Housing Inspection program focuses on various aspects of housing conditions, including grass height, weeds, trees, refuse-garbage, sidewalks, deck/porch, gutters, paint, roof, windows, structural foundation, and graffiti (City of Ames, 2021). This program serves as a valuable reference for evaluating and assessing housing quality in the rural context, particularly in the Iowa region. However, despite the efforts made by programs like the Ames City Housing Inspection, the subjective nature of evaluating existing housing conditions and the limited availability of resources for thorough investigations still pose challenges.\nRead more here and here.\n\nU.S Department of Housing and Urban Development \n\nThe U.S. Department of Housing and Urban Development (HUD) plays a vital role in ensuring housing quality standards across the United States. In their assessment of housing quality, HUD examines various aspects of the building exterior, including the condition of the foundation, stairs, rails, porches, roof, gutters, exterior surfaces, and chimney. Additionally, HUD emphasizes the importance of evaluating lead-based paint on exterior surfaces to ensure the safety and well-being of residents. These guidelines provided by HUD serve as a valuable reference for assessing and maintaining housing quality standards, contributing to the overall improvement of housing conditions nationwide. \nRead more here.\n\nOrange County Minimum Housing Code  \n\nThe Orange County minimum housing code plays a crucial role in ensuring housing standards and safety within the county. As outlined in Section 20-34 of the code, specific requirements include the maintenance of a safe foundation, stairs, walls, roofs, and porches. The code also addresses concerns such as high weeds and dilapidated vehicles, which can impact the overall quality of housing in the area. \nRead more here.\n\nTurning Data into Equity\n\nIn Detroit, 150 residents were tasked with surveying vacant land across the city that could be used. According to Data-Smart City Solutions, part of Harvard University, around 40,000 properties were found to be blighted and were subsequently given priority for either demolition or remediation. By involving residents in the surveying process, the city successfully identified a significant number of blighted properties, allowing for informed decision-making on demolition or remediation. Community engagement offers an avenue for prioritization, transparency, and alignment with local needs and aspirations.\n\nRoadBotics\n\nRoadBotics has developed a technology using artificial intelligence that has the ability to analyze road imagery to then assess issues and produce cost effective solutions. This allows cities to know when and where repairs need to take place, and deal with them while saving money. This type of technology also improves safety within cities as problems will not go unnoticed. \nRead more here. \n\nMason City Assessment\n\nAccording to Tara Brueggeman, an Assessor for Mason City, data is primarily sourced from publicly available platforms such as Beacon and the Vanguard assessment data management system. While web scraping is not feasible, city and county assessor offices can generate customized reports using SQL from Vanguard.\nDuring the appraisal process, appraisers rely on blueprints for property details and conduct in-person visits to measure and inspect properties. Interior inspections are now limited due to homeowner preferences, but the condition of the exterior serves as a valuable indicator. To minimize bias, appraisers adhere to guidelines outlined in the Iowa Real Property appraisal manual, avoid assessments during unfavorable personal circumstances, and undergo a review process by another individual before finalizing assessment records. These efforts contribute to the objectivity and accuracy of property assessments in Mason City.\n\nDes Moines Neighborhood Revitalization Program\n\nThe Des Moines Revitalization program is pioneering a groundbreaking approach to assess housing conditions through the utilization of AI. With the aim of revitalizing the city’s housing stock, this innovative program leverages AI technology to objectively evaluate the condition of homes in Des Moines. By analyzing various data points, including structural features, maintenance history, and overall quality, the AI system provides a comprehensive assessment of each property. This data-driven approach allows the program to identify areas in need of improvement, prioritize revitalization efforts, and allocate resources effectively.\nRead more here and here.\n\n\n\nThe image below depicts our project plan, outlining the strategies and actions we undertook to accomplish our objectives. The strategies include image gathering / data collection, creating a database and building AI models.\n\n\n\n\nWe first gathered house images manually from Kaggle, amassing a total of approximately 35,000 images. Kaggle is a website for hosting and sharing datasets. This website has many useful datasets for building AI models, which is why we decided to use images from Kaggle to aid in the initial AI model building process.\nHowever, since we needed more relevant images specific to Iowa, we decided to web scrape from other sources. Through this technique, we automated the process of acquiring images. Despite this improvement, we still had to manually sift through the gathered images, distinguishing those of high quality and suitable features for training our model from those of poorer quality.\nBased on the WINVEST project, the team decided to web scrape housing images and data from the communities of Grundy Center, New Hampton and Independence.\n\n\nWeb scraping is like a digital tool that automatically collects information from websites, allowing us to gather data quickly without manual searching.\nBefore starting any web scraping, our team had a plan to scrape the following websites for image, address, and attribute data:\n\nZillow\nZillow is an online platform and app for real estate that allows users to search for homes, view property listings, and access data on home values and market trends.\nTrulia\nTrulia is another online platform and app for real estate that helps users find homes, apartments, and other property listings, and provides information on neighborhoods and market trends.\nRealtor.com\nRealtor.com is a website and app that offers a comprehensive database of property listings, allowing users to search for homes, apartments, and other real estate options. It also provides resources for finding real estate agents.\nCounty Assessor Websites\nThese are online portals provided by local government entities that offer property-related information, including property records, tax assessments, and maps. Users can access details about property ownership and tax information.\nVanguard Appraisals, Inc\nVanguard Appraisals, Inc is a company that specializes in providing property appraisal services and software solutions for assessment in cities, counties and township Assessment Offices.-\nBeacon by Schneider Geospatial\nThis is a software platform used for property assessment and tax administration. It helps government agencies manage property data, valuation processes, and tax assessments by offering geospatial mapping, data analytics, and reporting capabilities.\nGoogle Street View\nGoogle Street View is a feature that provides 360-degree panoramic views of streets and locations worldwide. It allows users to virtually explore places, landmarks, and even remote areas, offering a unique visual experience from the comfort of their screens.\n\nUpon further reflection, we realized that Zillow owns Trulia. The major difference in the two sites is the Zestimate provided on Zillow. Because we were interested in the housing value, we chose to scrape Zillow out of the two.\nThe following are the sites we did scrape:\n\nZillow\nPolk County Assessor Website\nVanguard\nBeacon\nGoogle Street View\n\n\n\n\nVanguard has a GIS tool available that allowed us to select portions of cities and download addresses and parcel numbers. Only data for Independence was available through Vanguard. A downside to this method was that Vanguard limits the number of parcels we can select for download at a time.\nBeacon has more protection against web scraping than Vanguard. However, web scraping is necessary for the automation of downloading data on this website, as the GIS tool is not available. To scrape addresses and parcel numbers, and other house information for the remaining cities, we used the Instant Data Scraper Tool. This scraper provided us with CSV files of scraped data from Beacon.\nOnce the data had been scraped, we found that the CSV files provided us with a format that was not ideal.\nExamples of issues we encountered with the CSV files:\n\nMany of the cells contained hidden characters.\nNew lines within the cells.\n\nTo fix this file format, we used Excel’s Text-to-Columns tool to organize text, and Excel functions to separate and combine sections, and the Excel Find and Replace tool to remove unwanted characters.\nIn addition to re-formatting data, it was also necessary to create links for the Google Street View image collection process. Below is an example of a link to a Google Street View Image (excluding Google API key). To create links, Excel functions were used to concatenate the beginning part of the links (https://maps.googleapis.com/maps/api/streetview?size=800x800&location=), the address (303+I+AVE,+GRUNDY+CENTER+IOWA) and Google API key.\nhttps://maps.googleapis.com/maps/api/streetview?size=800x800&location=303+I+AVE,+GRUNDY+CENTER+IOWA\nThe addresses in the CSV files were the main source for creating Google API links, our main source for images. If the addresses are incorrect, we wouldn’t be able to pull the images.\nLink to more information here.\n\n\n\nOut of our five scraping sites, we got images from Zillow and Google Street View. Both sites presented challenges in gathering images. Because of the challenges, we decided to use WINVEST photos as well. In total, the image data came from four sites:\n\nZillow\nGoogle Street View\nWINVEST\nVanguard\n\n\n\n\nBefore we collected images, we created a standardized naming convention to maintain efficient organization. The base of our naming convention is source_city_address. In general, the first letter of the source or city represents it. Independence and New Hampton were adjusted to use the letter “D”and “H,” respectively.\n\n\n\nSource\nCity\n\n\n\n\nZ - Zillow\nH - New Hampton\n\n\nG - Google\nD - Independence\n\n\nV - Vanguard\nG - Grundy Center\n\n\nB - Beacon\nS - Slater\n\n\nW - WINVEST\n\n\n\n\nHere is an example of the naming convention in use:\n\nSource: Zillow\nCity: New Hampton, Iowa\nAddress: 311 W Main St\nResult: Z_H_311 W MAIN ST_\n\n\n\n\nWe scraped and downloaded image URLs for For Sale houses and recently Sold houses on Zillow. In total, we could scrape about one hundred images from Zillow. There were many more images available on Zillow that we couldn’t scrape.\nChallenges and Limitations of Scraping Zillow\n\nGood images are only available for For Sale and recently Sold houses. Other house images on Zillow are taken from Google.\nImages are stored in a carousel. The only image we could scrape was the first, which wasn’t always an image of the exterior.\nLazy loading. The term lazy loading refers to how a webpage loads its data. Because Zillow is lazy, it only loads the image data when the user scrolls down the page. When web scraping, only the loaded images are available. A more complicated process is needed to scrape a lazily loaded webpage.\n\nZillow is a great source for image data if you can get around the lazy loading issue and grab all images from the carousel. Zillow images for For Sale homes are more recent than other sources.\n\n\nA Google API key is required to scrape images from Google Street View. It is included in the image URL to allow viewing of a specific house.\nAs discussed earlier, the address data scraped from Vanguard and Beacon was cleaned and stored in CSV files to create the image URLs for use with Google Street View. We wrote code to open each image URL in the CSV file and download the image.\nHere is an example of the code to download the image from Google Street View.\n\ndata &lt;- read.csv(\"file_path\")\n\nurls_start &lt;- data[, 1]\n\nurls_full &lt;- paste(urls_start, “&key=”, sep = ““)\n\nurls_full_api_key &lt;- paste(urls_full, api_key, sep = ““)\n\n# creates folder and downloads all images\n\ndir.create(“google_images_folder_name”)\n\nfor(i in seq_along(urls_full_api_key)) {\n\n___ file_path &lt;- file.path(“google_images_folder_name”, paste0(“image_name”, data[i,6], “_.png”))\n\n___ download.file(urls_full_api_key[i], file_path, mode = “wb”)\n\n___ print(file_path)\n\n___ print(i)\n\n}\n\nClick here for more information.\nChallenges of Scraping Google Street View\nDuring the process of gathering images from Google Street View, we encountered the following problems:\n\nImage downloading takes time. It takes upwards of an hour for a city of 2,000.\nBlurred houses. Certain homeowners request Google to intentionally blur their residences on Google Street View to protect their privacy. Such images were ignored.\nAddress inconsistencies in Independence that caused errors when scraping. Some houses listed multiple house numbers, such as 100/101, while others had addresses like 100 1/2.\nDuplicate images for different addresses.\nParticular streets were not mapped, particularly in New Hampton, resulting in no image available.\nImages of inside of stores showing as an exterior house image.\n\n\n\n\n\n\n\n\n\n\n\nClick here for more information on collecting images from Google Street View.\n\n\n\n\n\n\nA spider is an automated version of web scraping that automatically goes page by page on a website. A spider reduces manual involvement in the scraping process and makes it more efficient.\nAaron Case, our friend on the AI Grocery Team, built us a spider to scrape Vanguard data. We were able to grab the following information from Vanguard:\n\nParcel number\nImage URL\nHouse style\nYear built\nSquare footage\nAppraised value\n\nClick here for more information on how Spider works.\n\n\n\n\n\nThis section demonstrates the potential of AI to recognize the quality of house.\nDetails of the first AI model can be found here.\n\nBuilding a Binary Image Classification AI Model - Binary Image Classification\nBuilding a Multi-Category Image Classification AI Model - Multi-Category Image Classification\nSorting Images - Image Sorting\nTraining the Model - Model Training\nUtilizing a Trained AI Model - Utilizing AI Model\nExporting Predictions to a CSV on Address - Export Predictions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Geographic Information System (GIS) is a spatial system that is used to analyze, display and store geographically referenced information. GIS uses data that is attached to a unique location and can be useful for identifying problems and trends.\nMore information here.\nMapping our house quality output using GIS enables us to visualize the AI-model output for vegetation, siding, gutters, and roof characteristics. This will allow us to visualize locations of houses in good conditions versus poor condition.\nIn the future, it could be beneficial to utilize statistical analysis techniques to understand the spatial relationship between data and visualize clusters.\n\n\nTo visualize addresses for the communities, first we needed to geocode (provide latitude and longitude for each address). There are many types of software that can be used to geocode addresses.\nQGIS is an open-source GIS software. We first geocoded addresses by using the QGIS Plugin called MMQGIS. When using this method, the plugin would crash and fail to geocode all the addresses. Because of this, we geocoded addresses using R instead. Base code was from storybench.\nBelow is an image of a CSV file with addresses latitude and longitude.\n\nMore information here.\n\nWe later realized we can use Tableau for better data connections and visualizations. We created a step by step process for creating a tableau dashboard showing the output from the AI Models and mapping it to the already geocoded addresses.\nClick here for more information.\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifying housing issues in a rural community requires more than just describing the physical and locational characteristics of its housing stock. A more complete understanding of local housing issues also requires consideration of the community’s residents. After consulting with our clients, we selected a set of demographic and economic measures to characterize the local population in our study communities. These indicators allow communties to put their local housing conditions within a broader context of peer cities.\n\n\nWe constructed a demographic profile of the three WINVEST cities - Grundy Center, Independence, and New Hampton - using R Studio. A demographic profile is a type of demographic analysis that focuses on a specific area. They are built using demographic data. Demographic data are but are not limited to, data on age, race, ethnicity, gender, marital status, income, education, and employment.\nThe following demographic data were used for the demographic profile of the communities:\n\nPopulation.\nMedian income\nMedian home value\nMedian year home built\nHome ownership rates\n\nWe chose these demographic data to give a well-rounded analysis of the housing demographics in our chosen communities and add context to the housing conditions we gathered.\nOur data came from the United States Decennial Census and the American Community Survey (ACS). The U.S. census is one of the best places to gather demographic data because it counts every resident in the United States. It has a very low inaccuracy, but the U.S. census is only conducted every ten years. The American Community Survey (ACS) is your best bet for more recent data. The ACS has more detailed demographic data and is conducted every five years for the entirety of the United States and every year for places with a population over 65,000. The ACS does not count every individual resident in the United States and instead relies on surveying a proportion of the population to create estimates of the demographics. Thus, it can be inaccurate and provides a margin of error. It is best used for data on the changing population, housing, and workforce.\nTo collect data in R Studio, we installed the Tidyverse and Tidycensus packages and loaded them with the library() function.\n\ninstall.packages(\"tidyverse\") \ninstall.packages(\"tidycensus\")\n\n\nlibrary(tidyverse) \nlibrary(tidycensus)\n\nThe Tidyverse package includes a range of functions that make coding in R Studio more user-friendly. It is not necessary for data collection, but it certainly does help. In contrast, the Tidycensus package is essential to data collection. The Tidycensus package lets you pull data directly from the Decennial Census and the ACS using the get_decennial() and get_acs() functions. You only need to specify three arguments to pull data: geography, year, and variable.\n\nget_decennial(geography = \"state\",\n              variable = \"P001001\",\n              year = 2010)  \n\nget_acs(geography = \"region\",\n        variable = \"B19013_001\",\n        year = 2010)\n\nTo visualize data in R Studio, we installed and loaded more packages: ggplot2, scales, and ggthemes.\n\ninstall.packages(\"ggplot2\") \ninstall.packages(\"scales\") \ninstall.packages(\"ggthemes\")\n\n\nlibrary(ggplot2) \nlibrary(scales) \nlibrary(ggthemes)\n\nThe ggplot2 package is required to create a plot in R Studio. With ggplot2 you get access to the function ggplot(), which allows you to create visualizations. ggplot() takes the arguments data and mapping to create a plot. Mapping refers to the x and y coordinates of our data. There are various geoms included in the ggplot2 package that aid in creating a visualization. Here are some examples:\n\nUsing the packages above, the following plots were made in R Studio:\n\n\nGeom\nResult\n\n\n\n\ngeom_line()\nline graph\n\n\ngeom_point()\nscatter plot\n\n\ngeom_histogram()\nhistogram\n\n\ngeom_bar()\nbar chart\n\n\ngeom_col()\ncolumn chart\n\n\n\n\n\n\n\n\n\nUsing the Infographics feature on ArcGIS Pro, we also created community summaries of Grundy Center, Independence, and New Hampton. These visualizations\nGrundy Center\n\nIndependence\n\nNew Hampton\n\nTo make a community summary infographic you can check here for a guide.\n\nKey findings and insights. \nInterpretation of demographic patterns and their implications. \n\n\n\n\nTo keep this project moving into other years, we decided to identify study communities in Iowa that could benefit from this project. All data collection was completed in R Studio using the Tidycensus and Tidyverse packages to pull data from the Decennial Census and American Community Survey (ACS).\nThe following demographic data was collected:\n\nAge\nThe age of a community’s population can be very telling of its growth. Communities that have a higher percentage of their population over the age of sixty-five than their population under eighteen could have issues with the younger generations moving away. A high median age could indicate the same issue.\nPopulation Size\nSpecifically, we were looking for communities with a population between 500 and 10,000. Communities with smaller populations are this project’s main focus. We were particularly interested in any communities that had a declining population as that can indicate it is struggling.\nHousing\nA low median house value and high median house age can indicate a struggling community. A high median house age can reveal that new housing is not being built in the community, and the community is not growing.\nOccupancy rates\nCommunities with low home ownership rates and high vacancy rates could have issues and be struggling. There is little room for community investment if residents cannot afford to buy their own homes.\nIncome\nA low median income or a low percent change in income can indicate a struggling community. There is little room for community investment when residents have little income.\nJobs and employment\nEmployment statistics are important because there will only be money to invest in a community if people are employed. The percentage of the workforce that commuted to work gives information on where jobs are located. If people are commuting, there are not enough jobs in town. People also tend to move to where jobs are.\n\nWe were able to start the visualization process in Tableau for the population data. From this visualization we can pin point the communities in Iowa that have a concerning growth rate and population size.\n[insert viz]\n\n\n\n\nIn conclusion, this project successfully achieved its objectives. A comprehensive literature review was conducted to explore the implementation of AI in planning and housing, providing valuable examples and insights. Study communities within the targeted population range were identified based on specific criteria, including population stability, the presence of a school, and median age of residents. Housing data, including images, was collected from reliable sources such as Google and county assessor websites.\nFurthermore, the project employed AI analysis, particularly siding, vegetation, gutter, and roof detection, to identify homes that could benefit from community assistance programs. The results were assessed and used to refine the selection criteria for Year 2, incorporating the gained insights.\n\n\n\n\nBased on the image sources used in this project, which are Kaggle, Google Street View, Zillow, Vanguard, Beacon, County Assessor Office/websites, the best image source is Google.\nIn Year 2, utilize county assessor websites as they provide valuable data for assessment purposes and assess to Zillow, Beacon, Vanguard, and so on.\nBox is best for storing images while Github is best for storing files. Google Drive will need to separately download images and files to run. Microsoft Teams is good for general quick sharing of information but not a great home for important documents.\nImage Classification for Model Training\n\n\n\n\n\n\n\nIf Image Classification is …\nthen we recommend …\n\n\nTime consuming\nTeamwork\n\n\nSubjective\nObtain two ratings per photo\n\n\nHas inconsistent ratings across evaluators\nObtain two ratings per photo\n\n\nResults influenced by number of ratings categories\nMake duplicates\n\n\nIt takes up space on your local storage\nDownload box on your your local computer\n\n\n\nCollect more training and test house images.\nBuild a broken window model to identify broken or boarded up windows and a trash model to identify trash in the yard.\nLearning materials on DataCamp\nDeep Learning with PyTorch\nIntroduction to Deep Learning with Keras\nImage Processing in Python\nArtificial Intelligence (AI) Concepts in Python\nIntroduction to Web Scraping in R\nUnderstanding machine learning\nIntroduction to Tableau\nImplement F1 Score into the code. It is a popular way to see how well models are performing.\nTools used for geocoding were QGIS and R, use python for geocoding to check if it will be better than the two used in the project.\nAdd data relating to accessibility to amenities, that could add valuable information to the analysis.\n\nMore objectives to be considered for Year 2 of this project;\n\nCould we use AI to determine which communities could benefit most from our project? If we tell it what characteristics of communities make it cause for concern, it could draw the conclusions for us and determine which communities we would want to reach out to.\nCreate an application that can be used by cities.\nLeveraging spatial analysis tools, we can gain valuable insights and broaden the analytical capabilities specifically in relation to identifying communities that might benefit.\nLook more into how to use Geospatial Artificial Intelligence (GeoAI).\nCreate AI model that can identify houses, trees, sidewalks and evaluate roof condition when given an image of a neighborhood.\n\n\n\n\n\n\n2000 - 2020 Decennial Census\n2021 5-Year American Community Survey\nZillow\nGoogle Street View\nVanguard\nBeacon\nAdditional sources can be found here and here."
  },
  {
    "objectID": "posts/Report_Draft/Report_Draft.html#introduction",
    "href": "posts/Report_Draft/Report_Draft.html#introduction",
    "title": "Housing Project Report",
    "section": "",
    "text": "Crucial to Midwestern rural vitality is a supply of good quality housing for residents of all ages and income levels. Crucial to effective rural housing policy development is a supply of good quality data describing local housing conditions.\nIdeally, communities would have ready access to information about their local housing stock to identify needs, set priorities, and optimize allocation of resources and investments. Unfortunately, many small communities lack the capacity to systematically evaluate their housing stock.\nCity and county property assessors’ offices seem a logical source for obtaining local housing data; however, tapping into these assessor databases poses some challenges. Access to a community’s full database may be restricted, as most city and county assessors now outsource data management to private firms. Even when accessible, the database may not be amenable to custom tabulations and targeted queries about specific housing conditions. In addition, the subjectivity of the assessment process introduces biases and inconsistences into the data themselves.\nData science approaches including Artificial Intelligence (AI) models offer great potential for addressing these and other housing data challenges encountered by rural communities and researchers who study them. For example, image classification models could be used to rate the condition of selected exterior housing features, such as roof and siding, or to detect the presence of problems such as missing or damaged gutters or an overgrown landscape. Leveraging AI technology in this manner could streamline the housing evaluation process, eliminate subjective biases, and facilitate informed decision-making for housing investment and development initiatives in rural communities."
  },
  {
    "objectID": "posts/Report_Draft/Report_Draft.html#goal-and-objectives",
    "href": "posts/Report_Draft/Report_Draft.html#goal-and-objectives",
    "title": "Housing Project Report",
    "section": "",
    "text": "The goal of this project is to investigate methods for conducting a thorough and objective evaluation of a community’s housing stock using AI, and also to explore how such methods could be adapted for multi-community analysis of relationships between housing quality and other local attributes in rural areas.\nObjectives:\n\nConduct a literature review to examine examples of AI applications in planning and housing.\nIdentify study communities and collect data about the communities.\nCollect housing data, including images, from existing sources such as realtor websites, Google Street View and county assessor websites.\nCollect data on housing age, number of bedrooms, square footage, and assessed value.\nUtilize roof, gutter, siding, and landscape condition as part of AI analysis to identify homes that may qualify for community assistance programs.\nAssess the results and recommend next steps for Year 2 of the project, considering the insights gained and community input received."
  },
  {
    "objectID": "posts/Report_Draft/Report_Draft.html#literature-review",
    "href": "posts/Report_Draft/Report_Draft.html#literature-review",
    "title": "Housing Project Report",
    "section": "",
    "text": "An important objective of the project was its focus on reviewing as much accessible literature and websites. Review of how cities have been using AI to address problems, and specifically housing problems, and how counties or cities assess housing conditions. The section also answers the question, what data might we use to assess housing condition in this project?\n\nAmes City Housing Inspection\n\nIn the context of housing inspections, the City of Ames in Iowa has implemented a comprehensive housing inspection program to ensure housing quality and safety standards are met. The Ames City Housing Inspection program focuses on various aspects of housing conditions, including grass height, weeds, trees, refuse-garbage, sidewalks, deck/porch, gutters, paint, roof, windows, structural foundation, and graffiti (City of Ames, 2021). This program serves as a valuable reference for evaluating and assessing housing quality in the rural context, particularly in the Iowa region. However, despite the efforts made by programs like the Ames City Housing Inspection, the subjective nature of evaluating existing housing conditions and the limited availability of resources for thorough investigations still pose challenges.\nRead more here and here.\n\nU.S Department of Housing and Urban Development \n\nThe U.S. Department of Housing and Urban Development (HUD) plays a vital role in ensuring housing quality standards across the United States. In their assessment of housing quality, HUD examines various aspects of the building exterior, including the condition of the foundation, stairs, rails, porches, roof, gutters, exterior surfaces, and chimney. Additionally, HUD emphasizes the importance of evaluating lead-based paint on exterior surfaces to ensure the safety and well-being of residents. These guidelines provided by HUD serve as a valuable reference for assessing and maintaining housing quality standards, contributing to the overall improvement of housing conditions nationwide. \nRead more here.\n\nOrange County Minimum Housing Code  \n\nThe Orange County minimum housing code plays a crucial role in ensuring housing standards and safety within the county. As outlined in Section 20-34 of the code, specific requirements include the maintenance of a safe foundation, stairs, walls, roofs, and porches. The code also addresses concerns such as high weeds and dilapidated vehicles, which can impact the overall quality of housing in the area. \nRead more here.\n\nTurning Data into Equity\n\nIn Detroit, 150 residents were tasked with surveying vacant land across the city that could be used. According to Data-Smart City Solutions, part of Harvard University, around 40,000 properties were found to be blighted and were subsequently given priority for either demolition or remediation. By involving residents in the surveying process, the city successfully identified a significant number of blighted properties, allowing for informed decision-making on demolition or remediation. Community engagement offers an avenue for prioritization, transparency, and alignment with local needs and aspirations.\n\nRoadBotics\n\nRoadBotics has developed a technology using artificial intelligence that has the ability to analyze road imagery to then assess issues and produce cost effective solutions. This allows cities to know when and where repairs need to take place, and deal with them while saving money. This type of technology also improves safety within cities as problems will not go unnoticed. \nRead more here. \n\nMason City Assessment\n\nAccording to Tara Brueggeman, an Assessor for Mason City, data is primarily sourced from publicly available platforms such as Beacon and the Vanguard assessment data management system. While web scraping is not feasible, city and county assessor offices can generate customized reports using SQL from Vanguard.\nDuring the appraisal process, appraisers rely on blueprints for property details and conduct in-person visits to measure and inspect properties. Interior inspections are now limited due to homeowner preferences, but the condition of the exterior serves as a valuable indicator. To minimize bias, appraisers adhere to guidelines outlined in the Iowa Real Property appraisal manual, avoid assessments during unfavorable personal circumstances, and undergo a review process by another individual before finalizing assessment records. These efforts contribute to the objectivity and accuracy of property assessments in Mason City.\n\nDes Moines Neighborhood Revitalization Program\n\nThe Des Moines Revitalization program is pioneering a groundbreaking approach to assess housing conditions through the utilization of AI. With the aim of revitalizing the city’s housing stock, this innovative program leverages AI technology to objectively evaluate the condition of homes in Des Moines. By analyzing various data points, including structural features, maintenance history, and overall quality, the AI system provides a comprehensive assessment of each property. This data-driven approach allows the program to identify areas in need of improvement, prioritize revitalization efforts, and allocate resources effectively.\nRead more here and here."
  },
  {
    "objectID": "posts/Report_Draft/Report_Draft.html#workflow",
    "href": "posts/Report_Draft/Report_Draft.html#workflow",
    "title": "Housing Project Report",
    "section": "",
    "text": "The image below depicts our project plan, outlining the strategies and actions we undertook to accomplish our objectives. The strategies include image gathering / data collection, creating a database and building AI models."
  },
  {
    "objectID": "posts/Report_Draft/Report_Draft.html#data-collection",
    "href": "posts/Report_Draft/Report_Draft.html#data-collection",
    "title": "Housing Project Report",
    "section": "",
    "text": "We first gathered house images manually from Kaggle, amassing a total of approximately 35,000 images. Kaggle is a website for hosting and sharing datasets. This website has many useful datasets for building AI models, which is why we decided to use images from Kaggle to aid in the initial AI model building process.\nHowever, since we needed more relevant images specific to Iowa, we decided to web scrape from other sources. Through this technique, we automated the process of acquiring images. Despite this improvement, we still had to manually sift through the gathered images, distinguishing those of high quality and suitable features for training our model from those of poorer quality.\nBased on the WINVEST project, the team decided to web scrape housing images and data from the communities of Grundy Center, New Hampton and Independence.\n\n\nWeb scraping is like a digital tool that automatically collects information from websites, allowing us to gather data quickly without manual searching.\nBefore starting any web scraping, our team had a plan to scrape the following websites for image, address, and attribute data:\n\nZillow\nZillow is an online platform and app for real estate that allows users to search for homes, view property listings, and access data on home values and market trends.\nTrulia\nTrulia is another online platform and app for real estate that helps users find homes, apartments, and other property listings, and provides information on neighborhoods and market trends.\nRealtor.com\nRealtor.com is a website and app that offers a comprehensive database of property listings, allowing users to search for homes, apartments, and other real estate options. It also provides resources for finding real estate agents.\nCounty Assessor Websites\nThese are online portals provided by local government entities that offer property-related information, including property records, tax assessments, and maps. Users can access details about property ownership and tax information.\nVanguard Appraisals, Inc\nVanguard Appraisals, Inc is a company that specializes in providing property appraisal services and software solutions for assessment in cities, counties and township Assessment Offices.-\nBeacon by Schneider Geospatial\nThis is a software platform used for property assessment and tax administration. It helps government agencies manage property data, valuation processes, and tax assessments by offering geospatial mapping, data analytics, and reporting capabilities.\nGoogle Street View\nGoogle Street View is a feature that provides 360-degree panoramic views of streets and locations worldwide. It allows users to virtually explore places, landmarks, and even remote areas, offering a unique visual experience from the comfort of their screens.\n\nUpon further reflection, we realized that Zillow owns Trulia. The major difference in the two sites is the Zestimate provided on Zillow. Because we were interested in the housing value, we chose to scrape Zillow out of the two.\nThe following are the sites we did scrape:\n\nZillow\nPolk County Assessor Website\nVanguard\nBeacon\nGoogle Street View\n\n\n\n\nVanguard has a GIS tool available that allowed us to select portions of cities and download addresses and parcel numbers. Only data for Independence was available through Vanguard. A downside to this method was that Vanguard limits the number of parcels we can select for download at a time.\nBeacon has more protection against web scraping than Vanguard. However, web scraping is necessary for the automation of downloading data on this website, as the GIS tool is not available. To scrape addresses and parcel numbers, and other house information for the remaining cities, we used the Instant Data Scraper Tool. This scraper provided us with CSV files of scraped data from Beacon.\nOnce the data had been scraped, we found that the CSV files provided us with a format that was not ideal.\nExamples of issues we encountered with the CSV files:\n\nMany of the cells contained hidden characters.\nNew lines within the cells.\n\nTo fix this file format, we used Excel’s Text-to-Columns tool to organize text, and Excel functions to separate and combine sections, and the Excel Find and Replace tool to remove unwanted characters.\nIn addition to re-formatting data, it was also necessary to create links for the Google Street View image collection process. Below is an example of a link to a Google Street View Image (excluding Google API key). To create links, Excel functions were used to concatenate the beginning part of the links (https://maps.googleapis.com/maps/api/streetview?size=800x800&location=), the address (303+I+AVE,+GRUNDY+CENTER+IOWA) and Google API key.\nhttps://maps.googleapis.com/maps/api/streetview?size=800x800&location=303+I+AVE,+GRUNDY+CENTER+IOWA\nThe addresses in the CSV files were the main source for creating Google API links, our main source for images. If the addresses are incorrect, we wouldn’t be able to pull the images.\nLink to more information here.\n\n\n\nOut of our five scraping sites, we got images from Zillow and Google Street View. Both sites presented challenges in gathering images. Because of the challenges, we decided to use WINVEST photos as well. In total, the image data came from four sites:\n\nZillow\nGoogle Street View\nWINVEST\nVanguard\n\n\n\n\nBefore we collected images, we created a standardized naming convention to maintain efficient organization. The base of our naming convention is source_city_address. In general, the first letter of the source or city represents it. Independence and New Hampton were adjusted to use the letter “D”and “H,” respectively.\n\n\n\nSource\nCity\n\n\n\n\nZ - Zillow\nH - New Hampton\n\n\nG - Google\nD - Independence\n\n\nV - Vanguard\nG - Grundy Center\n\n\nB - Beacon\nS - Slater\n\n\nW - WINVEST\n\n\n\n\nHere is an example of the naming convention in use:\n\nSource: Zillow\nCity: New Hampton, Iowa\nAddress: 311 W Main St\nResult: Z_H_311 W MAIN ST_\n\n\n\n\nWe scraped and downloaded image URLs for For Sale houses and recently Sold houses on Zillow. In total, we could scrape about one hundred images from Zillow. There were many more images available on Zillow that we couldn’t scrape.\nChallenges and Limitations of Scraping Zillow\n\nGood images are only available for For Sale and recently Sold houses. Other house images on Zillow are taken from Google.\nImages are stored in a carousel. The only image we could scrape was the first, which wasn’t always an image of the exterior.\nLazy loading. The term lazy loading refers to how a webpage loads its data. Because Zillow is lazy, it only loads the image data when the user scrolls down the page. When web scraping, only the loaded images are available. A more complicated process is needed to scrape a lazily loaded webpage.\n\nZillow is a great source for image data if you can get around the lazy loading issue and grab all images from the carousel. Zillow images for For Sale homes are more recent than other sources.\n\n\nA Google API key is required to scrape images from Google Street View. It is included in the image URL to allow viewing of a specific house.\nAs discussed earlier, the address data scraped from Vanguard and Beacon was cleaned and stored in CSV files to create the image URLs for use with Google Street View. We wrote code to open each image URL in the CSV file and download the image.\nHere is an example of the code to download the image from Google Street View.\n\ndata &lt;- read.csv(\"file_path\")\n\nurls_start &lt;- data[, 1]\n\nurls_full &lt;- paste(urls_start, “&key=”, sep = ““)\n\nurls_full_api_key &lt;- paste(urls_full, api_key, sep = ““)\n\n# creates folder and downloads all images\n\ndir.create(“google_images_folder_name”)\n\nfor(i in seq_along(urls_full_api_key)) {\n\n___ file_path &lt;- file.path(“google_images_folder_name”, paste0(“image_name”, data[i,6], “_.png”))\n\n___ download.file(urls_full_api_key[i], file_path, mode = “wb”)\n\n___ print(file_path)\n\n___ print(i)\n\n}\n\nClick here for more information.\nChallenges of Scraping Google Street View\nDuring the process of gathering images from Google Street View, we encountered the following problems:\n\nImage downloading takes time. It takes upwards of an hour for a city of 2,000.\nBlurred houses. Certain homeowners request Google to intentionally blur their residences on Google Street View to protect their privacy. Such images were ignored.\nAddress inconsistencies in Independence that caused errors when scraping. Some houses listed multiple house numbers, such as 100/101, while others had addresses like 100 1/2.\nDuplicate images for different addresses.\nParticular streets were not mapped, particularly in New Hampton, resulting in no image available.\nImages of inside of stores showing as an exterior house image.\n\n\n\n\n\n\n\n\n\n\n\nClick here for more information on collecting images from Google Street View.\n\n\n\n\n\n\nA spider is an automated version of web scraping that automatically goes page by page on a website. A spider reduces manual involvement in the scraping process and makes it more efficient.\nAaron Case, our friend on the AI Grocery Team, built us a spider to scrape Vanguard data. We were able to grab the following information from Vanguard:\n\nParcel number\nImage URL\nHouse style\nYear built\nSquare footage\nAppraised value\n\nClick here for more information on how Spider works."
  },
  {
    "objectID": "posts/Report_Draft/Report_Draft.html#ai-models",
    "href": "posts/Report_Draft/Report_Draft.html#ai-models",
    "title": "Housing Project Report",
    "section": "",
    "text": "This section demonstrates the potential of AI to recognize the quality of house.\nDetails of the first AI model can be found here.\n\nBuilding a Binary Image Classification AI Model - Binary Image Classification\nBuilding a Multi-Category Image Classification AI Model - Multi-Category Image Classification\nSorting Images - Image Sorting\nTraining the Model - Model Training\nUtilizing a Trained AI Model - Utilizing AI Model\nExporting Predictions to a CSV on Address - Export Predictions"
  },
  {
    "objectID": "posts/Report_Draft/Report_Draft.html#geographic-visualizations",
    "href": "posts/Report_Draft/Report_Draft.html#geographic-visualizations",
    "title": "Housing Project Report",
    "section": "",
    "text": "Mapping our output using GIS allows us to visualize and understand clusters for AI-model output for vegetation, siding, gutters, and roof. This will allow us visualize clusters or locations of houses in good conditions versus poor condition.\nTo visualize addresses for the communities, first we needed to geocode (latitude and longitude) each address. Base code was from storybench.\nMore information here.\n\nWe later realized we can use Tableau for better data connections and visualizations. We created a step by step process for creating a tableau dashboard showing the output from the AI Models and mapping it to the already geocoded addresses.\nClick here for more information."
  },
  {
    "objectID": "posts/Report_Draft/Report_Draft.html#demographic-analysis",
    "href": "posts/Report_Draft/Report_Draft.html#demographic-analysis",
    "title": "Housing Project Report",
    "section": "",
    "text": "Identifying housing issues in a rural community requires more than just describing the physical and locational characteristics of its housing stock. A more complete understanding of local housing issues also requires consideration of the community’s residents. After consulting with our clients, we selected a set of demographic and economic measures to characterize the local population in our study communities. These indicators allow communties to put their local housing conditions within a broader context of peer cities.\n\n\nWe constructed a demographic profile of the three WINVEST cities - Grundy Center, Independence, and New Hampton - using R Studio. A demographic profile is a type of demographic analysis that focuses on a specific area. They are built using demographic data. Demographic data are but are not limited to, data on age, race, ethnicity, gender, marital status, income, education, and employment.\nThe following demographic data were used for the demographic profile of the communities:\n\nPopulation.\nMedian income\nMedian home value\nMedian year home built\nHome ownership rates\n\nWe chose these demographic data to give a well-rounded analysis of the housing demographics in our chosen communities and add context to the housing conditions we gathered.\nOur data came from the United States Decennial Census and the American Community Survey (ACS). The U.S. census is one of the best places to gather demographic data because it counts every resident in the United States. It has a very low inaccuracy, but the U.S. census is only conducted every ten years. The American Community Survey (ACS) is your best bet for more recent data. The ACS has more detailed demographic data and is conducted every five years for the entirety of the United States and every year for places with a population over 65,000. The ACS does not count every individual resident in the United States and instead relies on surveying a proportion of the population to create estimates of the demographics. Thus, it can be inaccurate and provides a margin of error. It is best used for data on the changing population, housing, and workforce.\nTo collect data in R Studio, we installed the Tidyverse and Tidycensus packages and loaded them with the library() function.\n\ninstall.packages(\"tidyverse\") \ninstall.packages(\"tidycensus\")\n\n\nlibrary(tidyverse) \nlibrary(tidycensus)\n\nThe Tidyverse package includes a range of functions that make coding in R Studio more user-friendly. It is not necessary for data collection, but it certainly does help. In contrast, the Tidycensus package is essential to data collection. The Tidycensus package lets you pull data directly from the Decennial Census and the ACS using the get_decennial() and get_acs() functions. You only need to specify three arguments to pull data: geography, year, and variable.\n\nget_decennial(geography = \"state\",\n              variable = \"P001001\",\n              year = 2010)  \n\nget_acs(geography = \"region\",\n        variable = \"B19013_001\",\n        year = 2010)\n\nTo visualize data in R Studio, we installed and loaded more packages: ggplot2, scales, and ggthemes.\n\ninstall.packages(\"ggplot2\") \ninstall.packages(\"scales\") \ninstall.packages(\"ggthemes\")\n\n\nlibrary(ggplot2) \nlibrary(scales) \nlibrary(ggthemes)\n\nThe ggplot2 package is required to create a plot in R Studio. With ggplot2 you get access to the function ggplot(), which allows you to create visualizations. ggplot() takes the arguments data and mapping to create a plot. Mapping refers to the x and y coordinates of our data. There are various geoms included in the ggplot2 package that aid in creating a visualization. Here are some examples:\n\nUsing the packages above, the following plots were made in R Studio:\n\n\nGeom\nResult\n\n\n\n\ngeom_line()\nline graph\n\n\ngeom_point()\nscatter plot\n\n\ngeom_histogram()\nhistogram\n\n\ngeom_bar()\nbar chart\n\n\ngeom_col()\ncolumn chart\n\n\n\n\n\n\n\n\n\nUsing the Infographics feature on ArcGIS Pro, we also created community summaries of Grundy Center, Independence, and New Hampton. These visualizations\nGrundy Center\n\nIndependence\n\nNew Hampton\n\nTo make a community summary infographic you can check here for a guide.\n\nKey findings and insights. \nInterpretation of demographic patterns and their implications. \n\n\n\n\nTo keep this project moving into other years, we decided to identify study communities in Iowa that could benefit from this project. All data collection was completed in R Studio using the Tidycensus and Tidyverse packages to pull data from the Decennial Census and American Community Survey (ACS).\nThe following demographic data was collected:\n\nAge\nThe age of a community’s population can be very telling of its growth. Communities that have a higher percentage of their population over the age of sixty-five than their population under eighteen could have issues with the younger generations moving away. A high median age could indicate the same issue.\nPopulation Size\nSpecifically, we were looking for communities with a population between 500 and 10,000. Communities with smaller populations are this project’s main focus. We were particularly interested in any communities that had a declining population as that can indicate it is struggling.\nHousing\nA low median house value and high median house age can indicate a struggling community. A high median house age can reveal that new housing is not being built in the community, and the community is not growing.\nOccupancy rates\nCommunities with low home ownership rates and high vacancy rates could have issues and be struggling. There is little room for community investment if residents cannot afford to buy their own homes.\nIncome\nA low median income or a low percent change in income can indicate a struggling community. There is little room for community investment when residents have little income.\nJobs and employment\nEmployment statistics are important because there will only be money to invest in a community if people are employed. The percentage of the workforce that commuted to work gives information on where jobs are located. If people are commuting, there are not enough jobs in town. People also tend to move to where jobs are.\n\nWe were able to start the visualization process in Tableau for the population data. From this visualization we can pin point the communities in Iowa that have a concerning growth rate and population size.\n[insert viz]"
  },
  {
    "objectID": "posts/Report_Draft/Report_Draft.html#conclusion",
    "href": "posts/Report_Draft/Report_Draft.html#conclusion",
    "title": "Housing Project Report",
    "section": "",
    "text": "In conclusion, this project successfully achieved its objectives. A comprehensive literature review was conducted to explore the implementation of AI in planning and housing, providing valuable examples and insights. Study communities within the targeted population range were identified based on specific criteria, including population stability, the presence of a school, and median age of residents. Housing data, including images, was collected from reliable sources such as Google and county assessor websites.\nFurthermore, the project employed AI analysis, particularly siding, vegetation, gutter, and roof detection, to identify homes that could benefit from community assistance programs. The results were assessed and used to refine the selection criteria for Year 2, incorporating the gained insights."
  },
  {
    "objectID": "posts/Report_Draft/Report_Draft.html#recommendations",
    "href": "posts/Report_Draft/Report_Draft.html#recommendations",
    "title": "Housing Project Report",
    "section": "",
    "text": "Based on the image sources used in this project, which are Kaggle, Google Street View, Zillow, Vanguard, Beacon, County Assessor Office/websites, the best image source is Google.\nIn Year 2, utilize county assessor websites as they provide valuable data for assessment purposes and assess to Zillow, Beacon, Vanguard, and so on.\nBox is best for storing images while Github is best for storing files. Google Drive will need to separately download images and files to run. Microsoft Teams is good for general quick sharing of information but not a great home for important documents.\nImage Classification for Model Training\n\n\n\n\n\n\n\nIf Image Classification is …\nthen we recommend …\n\n\nTime consuming\nTeamwork\n\n\nSubjective\nObtain two ratings per photo\n\n\nHas inconsistent ratings across evaluators\nObtain two ratings per photo\n\n\nResults influenced by number of ratings categories\nMake duplicates\n\n\nIt takes up space on your local storage\nDownload box on your your local computer\n\n\n\nCollect more training and test house images.\nBuild a broken window model to identify broken or boarded up windows and a trash model to identify trash in the yard.\nLearning materials on DataCamp\nDeep Learning with PyTorch\nIntroduction to Deep Learning with Keras\nImage Processing in Python\nArtificial Intelligence (AI) Concepts in Python\nIntroduction to Web Scraping in R\nUnderstanding machine learning\nIntroduction to Tableau\nImplement F1 Score into the code. It is a popular way to see how well models are performing.\nTools used for geocoding were QGIS and R, use python for geocoding to check if it will be better than the two used in the project.\nAdd data relating to accessibility to amenities, that could add valuable information to the analysis.\n\nMore objectives to be considered for Year 2 of this project;\n\nCould we use AI to determine which communities could benefit most from our project? If we tell it what characteristics of communities make it cause for concern, it could draw the conclusions for us and determine which communities we would want to reach out to.\nCreate an application that can be used by cities.\nLeveraging spatial analysis tools, we can gain valuable insights and broaden the analytical capabilities specifically in relation to identifying communities that might benefit.\nLook more into how to use Geospatial Artificial Intelligence (GeoAI).\nCreate AI model that can identify houses, trees, sidewalks and evaluate roof condition when given an image of a neighborhood."
  },
  {
    "objectID": "posts/Report_Draft/Report_Draft.html#data-source",
    "href": "posts/Report_Draft/Report_Draft.html#data-source",
    "title": "Housing Project Report",
    "section": "",
    "text": "2000 - 2020 Decennial Census\n2021 5-Year American Community Survey\nZillow\nGoogle Street View\nVanguard\nBeacon\nAdditional sources can be found here and here."
  },
  {
    "objectID": "posts/Week_One/Week_One_Practice.html",
    "href": "posts/Week_One/Week_One_Practice.html",
    "title": "Progress Report",
    "section": "",
    "text": "The project’s initial phase involved establishing objectives that were aimed to be achieved. These include;\n\nUsing data to find good study cities;\nLiterature review on using AI in housing and planning.\nDemonstrate the potential of AI to address housing questions;\nFocus on gutter-related issues.\n\nCan we use street view images, or photos taken by the DSPG Team?\nCan we use the photos on the assessor database/website?\n\nWhat county data are available to assess housing conditions?\n\nWhat other data might we use to assess housing conditions?\n\nIdentify where local, state and/or federal funds might be best applied.\nTools that will be useful in Year 2 and 3.\n\nOur Progress\n\nLiterature Review\n\nWe conducted research on the methodologies employed by cities and counties in evaluating housing conditions. Our research encompassed examining Ames city, Des Moines city, the U.S. Department of Housing and Urban Development, Orange County, and Detroit. Additionally, we compiled research on how cities are using AI.\n\nImage Gathering / Data Collection\n\n\nZillow\nRealtors.com\nCounty Assessor Pages\n\nVanguard: Independence\nBeacon Schneider: Slater, New Hampton, and Grundy Center\nWe have obtained house images from the mentioned websites. Our focus is on four communities: Independence, Slater, New Hampton, and Grundy Center. However, during the process of web scraping to gather images, we encountered an issue with blurred houses. Further investigation revealed that certain homeowners pay Google to have their residences intentionally blurred on Google Street View.\n\nTo ensure convenient access to the images collected from various sources, we devised a standardized naming convention: Source, City, Address. For example,\n- Zillow\n- New Hampton, Iowa\n- 311 W Main St\nResult: Z_H_311 W MAIN ST_\nZ - Zillow\nG - Google\nV - Vanguard\nB - Beacon\nS - Slater\nH - New Hampton\nD - Independence\nG - Grundy Center\nDuring the data collection process, we encountered “address” inconsistencies in Independence. Some houses had multiple house numbers listed, such as 100/101, while others had addresses like 100 1/2. Fortunately, the function used to retrieve Google images managed to run successfully, although it reported 50 errors related to these address variations, including both the dual house numbers and the presence of half signs. It is worth noting that even if one of the address numbers is removed or the 1/2 is eliminated (essentially removing the “/” sign), the image URL still leads to a valid Google image. In order to address this issue, it is possible to revisit the URLs and modify them to obtain images corresponding to these specific addresses, if necessary.\n\nBuild, Train, and Test AI Models\n\n\nVegetation Model\nSiding Model\nGutter Model\n\nIn the initial stages of the project, we developed a vegetation model to gain insights before completing the housing data collection. This model utilizes artificial intelligence to analyze images and determine the presence or absence of vegetation in front of houses, providing a binary output of 0 (no vegetation) or 1 (vegetation detected).\nMoving forward, our objective is to construct additional AI models. The first model aims to identify whether a house is present in a given picture, assess if the image is obstructed by trees or cars, and determine its visibility. Additionally, this model will distinguish between images containing a single house or multiple houses.\nWe also plan to build models to address specific aspects such as vegetation, siding, and gutters. The vegetation model, which was developed earlier in the project, will be integrated to classify the level of vegetation as overgrown, maintained, or nonexistent. The siding model will assess whether the panels are broken or missing, if the paint has chipped off or faded, or if the siding is in good condition. Similarly, the gutter model will identify if the gutter is missing, damaged, or in good condition.\n\nCreate Database of Housing Information\n\n\nZillow\nRealtors.com\nCounty Assessor Pages\n\nVanguard: Independence\nBeacon Schneider: Slater, New Hampton, and Grundy Center\nCurrently, we have not established a comprehensive database to store all the data collected throughout the project. However, creating such a database would prove beneficial for the second year of the project. This centralized repository would enable efficient organization, management, and retrieval of the accumulated data, facilitating seamless access and analysis for future endeavors.\nTo summarize the task at-hand,\n\nIdentify study communities (places with population between 500 - 10,000, stable or increasing population, have a school, etc).\nCreate a database of housing information.\nSort images.\nBuild AI models.\nCompile all information in a detailed report."
  },
  {
    "objectID": "posts/Week_One/Week_One_Practice.html#housing-project-progress-report-year-1",
    "href": "posts/Week_One/Week_One_Practice.html#housing-project-progress-report-year-1",
    "title": "Progress Report",
    "section": "",
    "text": "The project’s initial phase involved establishing objectives that were aimed to be achieved. These include;\n\nUsing data to find good study cities;\nLiterature review on using AI in housing and planning.\nDemonstrate the potential of AI to address housing questions;\nFocus on gutter-related issues.\n\nCan we use street view images, or photos taken by the DSPG Team?\nCan we use the photos on the assessor database/website?\n\nWhat county data are available to assess housing conditions?\n\nWhat other data might we use to assess housing conditions?\n\nIdentify where local, state and/or federal funds might be best applied.\nTools that will be useful in Year 2 and 3.\n\nOur Progress\n\nLiterature Review\n\nWe conducted research on the methodologies employed by cities and counties in evaluating housing conditions. Our research encompassed examining Ames city, Des Moines city, the U.S. Department of Housing and Urban Development, Orange County, and Detroit. Additionally, we compiled research on how cities are using AI.\n\nImage Gathering / Data Collection\n\n\nZillow\nRealtors.com\nCounty Assessor Pages\n\nVanguard: Independence\nBeacon Schneider: Slater, New Hampton, and Grundy Center\nWe have obtained house images from the mentioned websites. Our focus is on four communities: Independence, Slater, New Hampton, and Grundy Center. However, during the process of web scraping to gather images, we encountered an issue with blurred houses. Further investigation revealed that certain homeowners pay Google to have their residences intentionally blurred on Google Street View.\n\nTo ensure convenient access to the images collected from various sources, we devised a standardized naming convention: Source, City, Address. For example,\n- Zillow\n- New Hampton, Iowa\n- 311 W Main St\nResult: Z_H_311 W MAIN ST_\nZ - Zillow\nG - Google\nV - Vanguard\nB - Beacon\nS - Slater\nH - New Hampton\nD - Independence\nG - Grundy Center\nDuring the data collection process, we encountered “address” inconsistencies in Independence. Some houses had multiple house numbers listed, such as 100/101, while others had addresses like 100 1/2. Fortunately, the function used to retrieve Google images managed to run successfully, although it reported 50 errors related to these address variations, including both the dual house numbers and the presence of half signs. It is worth noting that even if one of the address numbers is removed or the 1/2 is eliminated (essentially removing the “/” sign), the image URL still leads to a valid Google image. In order to address this issue, it is possible to revisit the URLs and modify them to obtain images corresponding to these specific addresses, if necessary.\n\nBuild, Train, and Test AI Models\n\n\nVegetation Model\nSiding Model\nGutter Model\n\nIn the initial stages of the project, we developed a vegetation model to gain insights before completing the housing data collection. This model utilizes artificial intelligence to analyze images and determine the presence or absence of vegetation in front of houses, providing a binary output of 0 (no vegetation) or 1 (vegetation detected).\nMoving forward, our objective is to construct additional AI models. The first model aims to identify whether a house is present in a given picture, assess if the image is obstructed by trees or cars, and determine its visibility. Additionally, this model will distinguish between images containing a single house or multiple houses.\nWe also plan to build models to address specific aspects such as vegetation, siding, and gutters. The vegetation model, which was developed earlier in the project, will be integrated to classify the level of vegetation as overgrown, maintained, or nonexistent. The siding model will assess whether the panels are broken or missing, if the paint has chipped off or faded, or if the siding is in good condition. Similarly, the gutter model will identify if the gutter is missing, damaged, or in good condition.\n\nCreate Database of Housing Information\n\n\nZillow\nRealtors.com\nCounty Assessor Pages\n\nVanguard: Independence\nBeacon Schneider: Slater, New Hampton, and Grundy Center\nCurrently, we have not established a comprehensive database to store all the data collected throughout the project. However, creating such a database would prove beneficial for the second year of the project. This centralized repository would enable efficient organization, management, and retrieval of the accumulated data, facilitating seamless access and analysis for future endeavors.\nTo summarize the task at-hand,\n\nIdentify study communities (places with population between 500 - 10,000, stable or increasing population, have a school, etc).\nCreate a database of housing information.\nSort images.\nBuild AI models.\nCompile all information in a detailed report."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Blog on Professional Contributions",
    "section": "",
    "text": "Housing Project Report\n\n\n\n\n\n\n\nReport Draft\n\n\nFinal Report\n\n\nPresentation\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2023\n\n\nAI Housing Team\n\n\n\n\n\n\n  \n\n\n\n\nDSPG Contributions Summary\n\n\n\n\n\n\n\nWeek One to Ten\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2023\n\n\nMorenike Atejioye\n\n\n\n\n\n\n  \n\n\n\n\nProgress Report\n\n\n\n\n\n\n\nProgress Report\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2023\n\n\nMorenike Atejioye\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "blog 2.html",
    "href": "blog 2.html",
    "title": "A Blog on Professional Contributions",
    "section": "",
    "text": "Housing Project Report\n\n\n\n\n\n\n\nReport Draft\n\n\nFinal Report\n\n\nPresentation\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2023\n\n\nAI Housing Team\n\n\n\n\n\n\n  \n\n\n\n\nDSPG Contributions Summary\n\n\n\n\n\n\n\nWeek One to Ten\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2023\n\n\nMorenike Atejioye\n\n\n\n\n\n\n  \n\n\n\n\nProgress Report\n\n\n\n\n\n\n\nProgress Report\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2023\n\n\nMorenike Atejioye\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "About",
    "section": "",
    "text": "This blog serves as a platform where I document my experiences and achievements during my participation in the DSPG summer program. Through this personal blog, I aim to showcase my notable contributions and delve into the impactful work I have undertaken in the program."
  },
  {
    "objectID": "posts/Report_Draft/Report_Draft.html#geographic-visualization",
    "href": "posts/Report_Draft/Report_Draft.html#geographic-visualization",
    "title": "Housing Project Report",
    "section": "",
    "text": "A Geographic Information System (GIS) is a spatial system that is used to analyze, display and store geographically referenced information. GIS uses data that is attached to a unique location and can be useful for identifying problems and trends.\nMore information here.\nMapping our house quality output using GIS enables us to visualize the AI-model output for vegetation, siding, gutters, and roof characteristics. This will allow us to visualize locations of houses in good conditions versus poor condition.\nIn the future, it could be beneficial to utilize statistical analysis techniques to understand the spatial relationship between data and visualize clusters.\n\n\nTo visualize addresses for the communities, first we needed to geocode (provide latitude and longitude for each address). There are many types of software that can be used to geocode addresses.\nQGIS is an open-source GIS software. We first geocoded addresses by using the QGIS Plugin called MMQGIS. When using this method, the plugin would crash and fail to geocode all the addresses. Because of this, we geocoded addresses using R instead. Base code was from storybench.\nBelow is an image of a CSV file with addresses latitude and longitude.\n\nMore information here.\n\nWe later realized we can use Tableau for better data connections and visualizations. We created a step by step process for creating a tableau dashboard showing the output from the AI Models and mapping it to the already geocoded addresses.\nClick here for more information."
  }
]