{
  "hash": "76ffc7d57735f0e6725861b11621955b",
  "result": {
    "markdown": "---\ntitle: \"Housing Project Report\"\nauthor: \"AI Housing Team\"\ndate: \"2023-07-14\"\ncategories: [Report Draft, Final Report, Presentation]\ntoc: TRUE\n---\n\n\n# **AI-Driven Housing Evaluation for Rural Community Development**\n\n------------------------------------------------------------------------\n\n![](/images/MicrosoftTeams-image%20(1)%202.png)\n\n------------------------------------------------------------------------\n\n## **Introduction**\n\nCrucial to Midwestern rural vitality is a supply of good quality housing for residents of all ages and income levels. Crucial to effective rural housing policy development is a supply of good quality data describing local housing conditions.\n\nIdeally, communities would have ready access to information about their local housing stock to identify needs, set priorities, and optimize allocation of resources and investments. Unfortunately, many small communities lack the capacity to systematically evaluate their housing stock.\n\nCity and county property assessors' offices seem a logical source for obtaining local housing data; however, tapping into these assessor databases poses some challenges. Access to a community's full database may be restricted, as most city and county assessors now outsource data management to private firms. Even when accessible, the database may not be amenable to custom tabulations and targeted queries about specific housing conditions. In addition, the subjectivity of the assessment process introduces biases and inconsistences into the data themselves.\n\nData science approaches including Artificial Intelligence (AI) models offer great potential for addressing these and other housing data challenges encountered by rural communities and researchers who study them. For example, image classification models could be used to rate the condition of selected exterior housing features, such as roof and siding, or to detect the presence of problems such as missing or damaged gutters or an overgrown landscape. Leveraging AI technology in this manner could streamline the housing evaluation process, eliminate subjective biases, and facilitate informed decision-making for housing investment and development initiatives in rural communities.\n\n## **Goal and Objectives**\n\nThe goal of this project is to investigate methods for conducting a thorough and objective evaluation of a community's housing stock using AI, and also to explore how such methods could be adapted for multi-community analysis of relationships between housing quality and other local attributes in rural areas.\n\n**Objectives:**\n\n-   Conduct a literature review to examine examples of AI applications in planning and housing.\n\n-   Identify study communities and collect data about the communities.\n\n-   Collect housing data, including images, from existing sources such as realtor websites, Google Street View and county assessor websites.\n\n-   Collect data on housing age, number of bedrooms, square footage, and assessed value.\n\n-   Utilize roof, gutter, siding, and landscape condition as part of AI analysis to identify homes that may qualify for community assistance programs.\n\n-   Assess the results and recommend next steps for Year 2 of the project, considering the insights gained and community input received.\n\n## **Literature Review**\n\nAn important objective of the project was its focus on reviewing as much accessible literature and websites. Review of how cities have been using AI to address problems, and specifically housing problems, and how counties or cities assess housing conditions. The section also answers the question, what data might we use to assess housing condition in this project?\n\n1.  **Ames City Housing Inspection**\n\nIn the context of housing inspections, the City of Ames in Iowa has implemented a comprehensive housing inspection program to ensure housing quality and safety standards are met. The Ames City Housing Inspection program focuses on various aspects of housing conditions, including grass height, weeds, trees, refuse-garbage, sidewalks, deck/porch, gutters, paint, roof, windows, structural foundation, and graffiti (City of Ames, 2021). This program serves as a valuable reference for evaluating and assessing housing quality in the rural context, particularly in the Iowa region. However, despite the efforts made by programs like the Ames City Housing Inspection, the subjective nature of evaluating existing housing conditions and the limited availability of resources for thorough investigations still pose challenges.\n\nRead more [here](https://www.cityofames.org/home/showpublisheddocument/53168/637087226539170000){target=\"_blank\"} and [here](https://jse.amstat.org/v19n3/decock/DataDocumentation.txt){target=\"_blank\"}.\n\n2.  **U.S Department of Housing and Urban Development** \n\nThe U.S. Department of Housing and Urban Development (HUD) plays a vital role in ensuring housing quality standards across the United States. In their assessment of housing quality, HUD examines various aspects of the building exterior, including the condition of the foundation, stairs, rails, porches, roof, gutters, exterior surfaces, and chimney. Additionally, HUD emphasizes the importance of evaluating lead-based paint on exterior surfaces to ensure the safety and well-being of residents. These guidelines provided by HUD serve as a valuable reference for assessing and maintaining housing quality standards, contributing to the overall improvement of housing conditions nationwide. \n\nRead more [here](https://www.hud.gov/sites/dfiles/OCHCO/documents/52580A.PDF){target=\"_blank\"}.\n\n3.  **Orange County Minimum Housing Code**  \n\nThe Orange County minimum housing code plays a crucial role in ensuring housing standards and safety within the county. As outlined in Section 20-34 of the code, specific requirements include the maintenance of a safe foundation, stairs, walls, roofs, and porches. The code also addresses concerns such as high weeds and dilapidated vehicles, which can impact the overall quality of housing in the area. \n\nRead more [here](https://library.municode.com/nc/orange_county/codes/code_of_ordinances?nodeId=PTIGEOR_CH20HO_ARTIIMIHOST){target=\"_blank\"}.\n\n4.  **Turning Data into Equity**\n\nIn Detroit, 150 residents were tasked with surveying vacant land across the city that could be used. According to [Data-Smart City Solutions](https://datasmart.ash.harvard.edu/news/article/battling-blight-four-ways-cities-are-using-data-to-address-vacant-propertie){target=\"_blank\"}, part of Harvard University, around 40,000 properties were found to be blighted and were subsequently given priority for either demolition or remediation. By involving residents in the surveying process, the city successfully identified a significant number of blighted properties, allowing for informed decision-making on demolition or remediation. Community engagement offers an avenue for prioritization, transparency, and alignment with local needs and aspirations.\n\n5.  **RoadBotics**\n\nRoadBotics has developed a technology using artificial intelligence that has the ability to analyze road imagery to then assess issues and produce cost effective solutions. This allows cities to know when and where repairs need to take place, and deal with them while saving money. This type of technology also improves safety within cities as problems will not go unnoticed. \n\nRead more [here](https://www.roadbotics.com/roadway-tutorial/){target=\"_blank\"}. \n\n6.  [Mason City](https://www.masoncityassessor.net/){target=\"_blank\"} **Assessment**\n\nAccording to Tara Brueggeman, an Assessor for Mason City, data is primarily sourced from publicly available platforms such as Beacon and the Vanguard assessment data management system. While web scraping is not feasible, city and county assessor offices can generate customized reports using SQL from Vanguard.\n\nDuring the appraisal process, appraisers rely on blueprints for property details and conduct in-person visits to measure and inspect properties. Interior inspections are now limited due to homeowner preferences, but the condition of the exterior serves as a valuable indicator. To minimize bias, appraisers adhere to guidelines outlined in the Iowa Real Property appraisal manual, avoid assessments during unfavorable personal circumstances, and undergo a review process by another individual before finalizing assessment records. These efforts contribute to the objectivity and accuracy of property assessments in Mason City.\n\n7.  **Des Moines Neighborhood Revitalization Program**\n\nThe Des Moines Revitalization program is pioneering a groundbreaking approach to assess housing conditions through the utilization of AI. With the aim of revitalizing the city's housing stock, this innovative program leverages AI technology to objectively evaluate the condition of homes in Des Moines. By analyzing various data points, including structural features, maintenance history, and overall quality, the AI system provides a comprehensive assessment of each property. This data-driven approach allows the program to identify areas in need of improvement, prioritize revitalization efforts, and allocate resources effectively.\n\nRead more [here](https://www.desmoinesregister.com/story/news/local/des-moines/2023/03/08/des-moines-wonders-if-a-machine-can-better-assess-home-conditions/69965121007/?utm_source=ground.news&utm_medium=referral){target=\"_blank\"} and [here](https://www.axios.com/local/des-moines/2023/03/10/ai-assessment-desmoines-properties-iowa){target=\"_blank\"}.\n\n## Workflow\n\nThe image below depicts our project plan, outlining the strategies and actions we undertook to accomplish our objectives. The strategies include image gathering / data collection, creating a database and building AI models.\n\n![](/images/project_plan_new2.png)\n\n## Data Collection\n\nWe first gathered house images manually from Kaggle, amassing a total of approximately 35,000 images. Kaggle is a website for hosting and sharing datasets. This website has many useful datasets for building AI models, which is why we decided to use images from Kaggle to aid in the initial AI model building process.\n\nHowever, since we needed more relevant images specific to Iowa, we decided to web scrape from other sources. Through this technique, we automated the process of acquiring images. Despite this improvement, we still had to manually sift through the gathered images, distinguishing those of high quality and suitable features for training our model from those of poorer quality.\n\nBased on the WINVEST project, the team decided to web scrape housing images and data from the communities of Grundy Center, New Hampton and Independence.\n\n### What is Web Scraping?\n\nWeb scraping is like a digital tool that automatically collects information from websites, allowing us to gather data quickly without manual searching.\n\nBefore starting any web scraping, our team had a plan to scrape the following websites for image, address, and attribute data:\n\n-   [**Zillow**](https://www.zillow.com/?utm_medium=cpc&utm_source=google&utm_content=1471764169%7C65545421228%7Caud-352785741564:kwd-570802407%7C603457706088%7C&semQue=null&gclid=EAIaIQobChMI-Nv7oPL3_wIVWSazAB0eyA9CEAAYASAAEgKaafD_BwE){target=\"_blank\"}\n\n    Zillow is an online platform and app for real estate that allows users to search for homes, view property listings, and access data on home values and market trends.\n\n-   [**Trulia**](https://www.trulia.com/){target=\"_blank\"}\n\n    Trulia is another online platform and app for real estate that helps users find homes, apartments, and other property listings, and provides information on neighborhoods and market trends.\n\n-   [**Realtor.com**](https://www.realtor.com/){target=\"_blank\"}\n\n    Realtor.com is a website and app that offers a comprehensive database of property listings, allowing users to search for homes, apartments, and other real estate options. It also provides resources for finding real estate agents.\n\n-   **County Assessor Websites**\n\n    These are online portals provided by local government entities that offer property-related information, including property records, tax assessments, and maps. Users can access details about property ownership and tax information.\n\n-   [**Vanguard Appraisals, Inc**](https://www.iowaassessors.com/){target=\"_blank\"}\n\n    Vanguard Appraisals, Inc is a company that specializes in providing property appraisal services and software solutions for assessment in cities, counties and township Assessment Offices.-\n\n-   [**Beacon**](https://beacon.schneidercorp.com/){target=\"_blank\"} **by Schneider Geospatial**\n\n    This is a software platform used for property assessment and tax administration. It helps government agencies manage property data, valuation processes, and tax assessments by offering geospatial mapping, data analytics, and reporting capabilities.\n\n-   [**Google Street View**](https://www.google.com/maps/@42.0275015,-93.6351493,14z?entry=ttu){target=\"_blank\"}\n\n    Google Street View is a feature that provides 360-degree panoramic views of streets and locations worldwide. It allows users to virtually explore places, landmarks, and even remote areas, offering a unique visual experience from the comfort of their screens.\n\nUpon further reflection, we realized that Zillow owns Trulia. The major difference in the two sites is the Zestimate provided on Zillow. Because we were interested in the housing value, we chose to scrape Zillow out of the two.\n\nThe following are the sites we did scrape:\n\n-   Zillow\n-   Polk County Assessor Website\n-   Vanguard\n-   Beacon\n-   Google Street View\n\n### Address Collection\n\nVanguard has a GIS tool available that allowed us to select portions of cities and download addresses and parcel numbers. Only data for Independence was available through Vanguard. A downside to this method was that Vanguard limits the number of parcels we can select for download at a time.\n\nBeacon has more protection against web scraping than Vanguard. However, web scraping is necessary for the automation of downloading data on this website, as the GIS tool is not available. To scrape addresses and parcel numbers, and other house information for the remaining cities, we used the [Instant Data Scraper Tool](https://chrome.google.com/webstore/detail/instant-data-scraper/ofaokhiedipichpaobibbnahnkdoiiah). This scraper provided us with CSV files of scraped data from Beacon.\n\nOnce the data had been scraped, we found that the CSV files provided us with a format that was not ideal.\n\nExamples of issues we encountered with the CSV files:\n\n-   Many of the cells contained hidden characters.\n\n-   New lines within the cells.\n\nTo fix this file format, we used Excel's Text-to-Columns tool to organize text, and Excel functions to separate and combine sections, and the Excel Find and Replace tool to remove unwanted characters.\n\nIn addition to re-formatting data, it was also necessary to create links for the Google Street View image collection process. Below is an example of a link to a Google Street View Image (excluding Google API key). To create links, Excel functions were used to concatenate the beginning part of the links (https://maps.googleapis.com/maps/api/streetview?size=800x800&location=), the address (303+I+AVE,+GRUNDY+CENTER+IOWA) and Google API key.\n\n**https://maps.googleapis.com/maps/api/streetview?size=800x800&location=303+I+AVE,+GRUNDY+CENTER+IOWA**\n\nThe addresses in the CSV files were the main source for creating Google API links, our main source for images. If the addresses are incorrect, we wouldn't be able to pull the images.\n\nLink to more information [here](https://gavinfishy.github.io/Gavin_DSPG_Blog/posts/Gavin-Fisher_guide_w7/full_guide.html#address-collection-and-cleaning){target=\"_blank\"}.\n\n### Image Collection\n\nOut of our five scraping sites, we got images from Zillow and Google Street View. Both sites presented challenges in gathering images. Because of the challenges, we decided to use WINVEST photos as well. In total, the image data came from four sites:\n\n-   Zillow\n\n-   Google Street View\n\n-   WINVEST\n\n-   Vanguard\n\n### Naming Convention\n\nBefore we collected images, we created a standardized naming convention to maintain efficient organization. The base of our naming convention is **source_city_address**. In general, the first letter of the source or city represents it. Independence and New Hampton were adjusted to use the letter \"D\"and \"H,\" respectively.\n\n| Source       | City              |\n|--------------|-------------------|\n| Z - Zillow   | H - New Hampton   |\n| G - Google   | D - Independence  |\n| V - Vanguard | G - Grundy Center |\n| B - Beacon   | S - Slater        |\n| W - WINVEST  |                   |\n\nHere is an example of the naming convention in use:\n\n> Source: Zillow\n>\n> City: New Hampton, Iowa\n>\n> Address: 311 W Main St\n>\n> **Result**: Z_H_311 W MAIN ST\\_\n\n### Web Scraping Zillow\n\nWe scraped and downloaded image URLs for For Sale houses and recently Sold houses on Zillow. In total, we could scrape about one hundred images from Zillow. There were many more images available on Zillow that we couldn't scrape.\n\n**Challenges and Limitations of Scraping Zillow**\n\n-   Good images are only available for For Sale and recently Sold houses. Other house images on Zillow are taken from Google.\n\n-   Images are stored in a carousel. The only image we could scrape was the first, which wasn't always an image of the exterior.\n\n-   Lazy loading. The term lazy loading refers to how a webpage loads its data. Because Zillow is lazy, it only loads the image data when the user scrolls down the page. When web scraping, only the loaded images are available. A more complicated process is needed to scrape a lazily loaded webpage.\n\nZillow is a great source for image data if you can get around the lazy loading issue and grab all images from the carousel. Zillow images for For Sale homes are more recent than other sources.\n\n#### Scraping Google Street View\n\nA Google API key is required to scrape images from Google Street View. It is included in the image URL to allow viewing of a specific house.\n\nAs discussed earlier, the address data scraped from Vanguard and Beacon was cleaned and stored in CSV files to create the image URLs for use with Google Street View. We wrote code to open each image URL in the CSV file and download the image.\n\nHere is an example of the code to download the image from Google Street View.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read.csv(\"file_path\")\n\nurls_start <- data[, 1]\n\nurls_full <- paste(urls_start, “&key=”, sep = ““)\n\nurls_full_api_key <- paste(urls_full, api_key, sep = ““)\n\n# creates folder and downloads all images\n\ndir.create(“google_images_folder_name”)\n\nfor(i in seq_along(urls_full_api_key)) {\n\n___ file_path <- file.path(“google_images_folder_name”, paste0(“image_name”, data[i,6], “_.png”))\n\n___ download.file(urls_full_api_key[i], file_path, mode = “wb”)\n\n___ print(file_path)\n\n___ print(i)\n\n}\n```\n:::\n\n\nClick [here](https://gavinfishy.github.io/Gavin_DSPG_Blog/posts/Gavin-Fisher_guide_w7/full_guide.html#creating-google-api-links){target=\"_blank\"} for more information.\n\n**Challenges of Scraping Google Street View**\n\nDuring the process of gathering images from Google Street View, we encountered the following problems:\n\n-   Image downloading takes time. It takes upwards of an hour for a city of 2,000.\n\n-   Blurred houses. Certain homeowners request Google to intentionally blur their residences on Google Street View to protect their privacy. Such images were ignored.\n\n-   Address inconsistencies in Independence that caused errors when scraping. Some houses listed multiple house numbers, such as 100/101, while others had addresses like 100 1/2.\n\n-   Duplicate images for different addresses.\n\n-   Particular streets were not mapped, particularly in New Hampton, resulting in no image available.\n\n-   Images of inside of stores showing as an exterior house image.\n\n::: {layout-ncol=\"2\"}\n![](/images/MicrosoftTeams-image%20(1).png){width=\"436\"}\n\n![](/images/no_image.png){width=\"436\"}\n:::\n\nClick [here](https://gavinfishy.github.io/Gavin_DSPG_Blog/posts/Gavin-Fisher_guide_w7/full_guide.html#scrape-google-images){target=\"_blank\"} for more information on collecting images from Google Street View.\n\n### Attribute Collection\n\n#### Using a Spider to Collect Vanguard Information\n\nA spider is an automated version of web scraping that automatically goes page by page on a website. A spider reduces manual involvement in the scraping process and makes it more efficient.\n\nAaron Case, our friend on the AI Grocery Team, built us a spider to scrape Vanguard data. We were able to grab the following information from Vanguard:\n\n-   Parcel number\n\n-   Image URL\n\n-   House style\n\n-   Year built\n\n-   Square footage\n\n-   Appraised value\n\nClick [here](https://cyberspeedac.github.io/Aaron-Case-DSPG-Blog/posts/Aaron_C_MultiWeek/Aaron_C_Spiders.html){target=\"_blank\"} for more information on how Spider works.\n\n## AI Models\n\nThis section demonstrates the potential of AI to recognize the quality of house.\n\nDetails of the first AI model can be found [here](https://gavinfishy.github.io/Gavin_DSPG_Blog/posts/Gavin-Fisher_week3/Week_Three.html){target=\"_blank\"}.\n\n-   Building a Binary Image Classification AI Model - [Binary Image Classification](https://gavinfishy.github.io/Gavin_DSPG_Blog/posts/Gavin-Fisher_guide_w7/full_guide.html#building-a-binary-image-classification-ai-model){target=\"_blank\"}\n\n-   Building a Multi-Category Image Classification AI Model - [Multi-Category Image Classification](https://gavinfishy.github.io/Gavin_DSPG_Blog/posts/Gavin-Fisher_guide_w7/full_guide.html#building-a-multi-category-image-classification-ai-model){target=\"_blank\"}\n\n-   Sorting Images - [Image Sorting](https://gavinfishy.github.io/Gavin_DSPG_Blog/posts/Gavin-Fisher_guide_w7/full_guide.html#sorting-images){target=\"_blank\"}\n\n-   Training the Model - [Model Training](https://gavinfishy.github.io/Gavin_DSPG_Blog/posts/Gavin-Fisher_guide_w7/full_guide.html#training-the-model){target=\"_blank\"}\n\n-   Utilizing a Trained AI Model - [Utilizing AI Model](https://gavinfishy.github.io/Gavin_DSPG_Blog/posts/Gavin-Fisher_guide_w7/full_guide.html#utilizing-a-trained-ai-model){target=\"_blank\"}\n\n-   Exporting Predictions to a CSV on Address - [Export Predictions](https://gavinfishy.github.io/Gavin_DSPG_Blog/posts/Gavin-Fisher_guide_w7/full_guide.html#exporting-predictions-to-a-csv-on-address){target=\"_blank\"}\n\n![](/images/Endgame.png)\n\n### [Guide to Housing Project AI Models](https://gavinfishy.github.io/Gavin_DSPG_Blog/posts/Gavin-Fisher_AI_guide_w9/AI_guide.html){target=\"_blank\"}\n\n![](/images/neural_network.png)\n\n### Confidence Level and Accuracy of Models\n\n![](/images/confidencelevelnew.png)\n\n## Geographic Visualization\n\n### What is GIS and how is it used in this project?\n\nA Geographic Information System (GIS) is a spatial system that is used to analyze, display and store geographically referenced information. GIS uses data that is attached to a unique location and can be useful for identifying problems and trends.\n\nMore information [here](https://www.esri.com/en-us/what-is-gis/overview).\n\nMapping our house quality output using GIS enables us to visualize the AI-model output for vegetation, siding, gutters, and roof characteristics. This will allow us to visualize locations of houses in good conditions versus poor condition.\n\nIn the future, it could be beneficial to utilize statistical analysis techniques to understand the spatial relationship between data and visualize clusters.\n\n#### Geocoding Addresses\n\nTo visualize addresses for the communities, first we needed to geocode (provide latitude and longitude for each address). There are many types of software that can be used to geocode addresses.\n\nQGIS is an open-source GIS software. We first geocoded addresses by using the QGIS Plugin called MMQGIS. When using this method, the plugin would crash and fail to geocode all the addresses. Because of this, we geocoded addresses using R instead. Base code was from [storybench](https://www.storybench.org/geocode-csv-addresses-r/){target=\"_blank\"}.\n\nBelow is an image of a CSV file with addresses latitude and longitude.\n\n![](/images/ogden_geocoded_addresses.png)\n\nMore information [here](https://1angelinaevans.github.io/AngelinaEvansBlog/posts/Week6TeamBlog/Week6TeamBlog.html#mapping){target=\"_blank\"}.\n\n![](/images/siding_slater.png)\n\nWe later realized we can use Tableau for better data connections and visualizations. We created a step by step process for creating a tableau dashboard showing the output from the AI Models and mapping it to the already geocoded addresses.\n\nClick [here](https://1angelinaevans.github.io/AngelinaEvansBlog/posts/MappingHouseData/MappingHouseQuality.html){target=\"_blank\"} for more information.\n\n### Tableau Dashboard\n\n![](/images/screenshot_dashboardnew.png){width=\"619\"}\n\n#### [Tableau Dashboard](https://1angelinaevans.github.io/AngelinaEvansBlog/posts/House%20Quality%20Dashboards/HouseQualityDashboard.html){target=\"_blank\"}\n\n## Demographic Analysis\n\nIdentifying housing issues in a rural community requires more than just describing the physical and locational characteristics of its housing stock. A more complete understanding of local housing issues also requires consideration of the community's residents. After consulting with our clients, we selected a set of demographic and economic measures to characterize the local population in our study communities. These indicators allow communties to put their local housing conditions within a broader context of peer cities.\n\n### Profiling Using R Studio\n\nWe constructed a demographic profile of the three WINVEST cities - Grundy Center, Independence, and New Hampton - using R Studio. A demographic profile is a type of demographic analysis that focuses on a specific area. They are built using demographic data. Demographic data are but are not limited to, data on **age**, **race**, **ethnicity**, **gender**, **marital status**, **income**, **education**, and **employment**.\n\nThe following demographic data were used for the demographic profile of the communities:\n\n-   Population.\n\n-   Median income\n\n-   Median home value\n\n-   Median year home built\n\n-   Home ownership rates\n\nWe chose these demographic data to give a well-rounded analysis of the housing demographics in our chosen communities and add context to the housing conditions we gathered.\n\nOur data came from the **United States Decennial Census** and the **American Community Survey (ACS)**. The U.S. census is one of the best places to gather demographic data because it counts every resident in the United States. It has a very low inaccuracy, but the U.S. census is only conducted every ten years. The American Community Survey (ACS) is your best bet for more recent data. The ACS has more detailed demographic data and is conducted every five years for the entirety of the United States and every year for places with a population over 65,000. The ACS does not count every individual resident in the United States and instead relies on surveying a proportion of the population to create estimates of the demographics. Thus, it can be inaccurate and provides a margin of error. It is best used for data on the changing population, housing, and workforce.\n\nTo collect data in R Studio, we installed the *Tidyverse* and *Tidycensus* packages and loaded them with the `library()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"tidyverse\") \ninstall.packages(\"tidycensus\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) \nlibrary(tidycensus)\n```\n:::\n\n\nThe *Tidyverse* package includes a range of functions that make coding in R Studio more user-friendly. It is not necessary for data collection, but it certainly does help. In contrast, the *Tidycensus* package is essential to data collection. The *Tidycensus* package lets you pull data directly from the Decennial Census and the ACS using the `get_decennial()` and `get_acs()` functions. You only need to specify three arguments to pull data: geography, year, and variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_decennial(geography = \"state\",\n              variable = \"P001001\",\n              year = 2010)  \n\nget_acs(geography = \"region\",\n        variable = \"B19013_001\",\n        year = 2010)\n```\n:::\n\n\nTo visualize data in R Studio, we installed and loaded more packages: *ggplot2, scales,* and *ggthemes.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"ggplot2\") \ninstall.packages(\"scales\") \ninstall.packages(\"ggthemes\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2) \nlibrary(scales) \nlibrary(ggthemes)\n```\n:::\n\n\n*The ggplot2* package is required to create a plot in R Studio. With *ggplot2* you get access to the function `ggplot(),` which allows you to create visualizations. `ggplot()` takes the arguments `data` and `mapping` to create a plot. `Mapping` refers to the x and y coordinates of our data. There are various geoms included in the *ggplot2* package that aid in creating a visualization. Here are some examples:\n\n| Geom             | Result       |\n|------------------|--------------|\n| geom_line()      | line graph   |\n| geom_point()     | scatter plot |\n| geom_histogram() | histogram    |\n| geom_bar()       | bar chart    |\n| geom_col()       | column chart |\n\n: Using the packages above, the following plots were made in R Studio:\n\n![](/images/demographics/all_percent_change.png){width=\"571\"}\n\n![](/images/demographics/pop_projection_cities.png){width=\"571\"}\n\n![](/images/demographics/home_ownership.png){width=\"571\"}\n\n![](/images/demographics/median_income.png)\n\n![](/images/demographics/median_value.png)\n\n![](/images/demographics/value_and_year_built.png){width=\"522\"}\n\nUsing the Infographics feature on ArcGIS Pro, we also created community summaries of Grundy Center, Independence, and New Hampton. These visualizations\n\n**Grundy Center**\n\n![](/images/grundy_center_community_summary%20(1).png){width=\"703\"}\n\n**Independence**\n\n![](/images/independence_community_summary.png){width=\"703\"}\n\n**New Hampton**\n\n![](/images/new_hampton_community_summary.png){width=\"703\"}\n\nTo make a community summary infographic you can check [here](https://kailynhogan.github.io/Kailyn_Hogan_DSPG_Blog/posts/Week_7/Week_7.html#monday){target=\"_blank\"} for a guide.\n\n3.  Key findings and insights. \n\n4.  Interpretation of demographic patterns and their implications. \n\n### Collecting Data to Identify Study Communities\n\nTo keep this project moving into other years, we decided to identify study communities in Iowa that could benefit from this project. All data collection was completed in R Studio using the Tidycensus and Tidyverse packages to pull data from the Decennial Census and American Community Survey (ACS).\n\nThe following demographic data was collected:\n\n-   **Age**\n\n    The age of a community's population can be very telling of its growth. Communities that have a higher percentage of their population over the age of sixty-five than their population under eighteen could have issues with the younger generations moving away. A high median age could indicate the same issue.\n\n-   **Population Size**\n\n    Specifically, we were looking for communities with a population between 500 and 10,000. Communities with smaller populations are this project's main focus. We were particularly interested in any communities that had a declining population as that can indicate it is struggling.\n\n-   **Housing**\n\n    A low median house value and high median house age can indicate a struggling community. A high median house age can reveal that new housing is not being built in the community, and the community is not growing.\n\n-   **Occupancy rates**\n\n    Communities with low home ownership rates and high vacancy rates could have issues and be struggling. There is little room for community investment if residents cannot afford to buy their own homes.\n\n-   **Income**\n\n    A low median income or a low percent change in income can indicate a struggling community. There is little room for community investment when residents have little income.\n\n-   **Jobs and employment**\n\n    Employment statistics are important because there will only be money to invest in a community if people are employed. The percentage of the workforce that commuted to work gives information on where jobs are located. If people are commuting, there are not enough jobs in town. People also tend to move to where jobs are.\n\nWe were able to start the visualization process in Tableau for the population data. From this visualization we can pin point the communities in Iowa that have a concerning growth rate and population size.\n\n\\[insert viz\\]\n\n## Conclusion\n\nIn conclusion, this project successfully achieved its objectives. A comprehensive literature review was conducted to explore the implementation of AI in planning and housing, providing valuable examples and insights. Study communities within the targeted population range were identified based on specific criteria, including population stability, the presence of a school, and median age of residents. Housing data, including images, was collected from reliable sources such as Google and county assessor websites.\n\nFurthermore, the project employed AI analysis, particularly siding, vegetation, gutter, and roof detection, to identify homes that could benefit from community assistance programs. The results were assessed and used to refine the selection criteria for Year 2, incorporating the gained insights.\n\n## Recommendations\n\n-   Based on the image sources used in this project, which are Kaggle, Google Street View, Zillow, Vanguard, Beacon, County Assessor Office/websites, the best image source is **Google**.\n\n    In Year 2, utilize county assessor websites as they provide valuable data for assessment purposes and assess to Zillow, Beacon, Vanguard, and so on.\n\n-   Box is best for storing images while Github is best for storing files. Google Drive will need to separately download images and files to run. Microsoft Teams is good for general quick sharing of information but not a great home for important documents.\n\n-   **Image Classification for Model Training**\n\n    |                                                    |                                          |\n    |------------------------------------|------------------------------------|\n    | **If Image Classification is ...**                 | **then we recommend ...**                |\n    | Time consuming                                     | Teamwork                                 |\n    | Subjective                                         | Obtain two ratings per photo             |\n    | Has inconsistent ratings across evaluators         | Obtain two ratings per photo             |\n    | Results influenced by number of ratings categories | Make duplicates                          |\n    | It takes up space on your local storage            | Download box on your your local computer |\n\n-   Collect more training and test house images.\n\n-   Build a broken window model to identify broken or boarded up windows and a trash model to identify trash in the yard.\n\n-   **Learning materials on DataCamp**\n\n    Deep Learning with PyTorch\n\n    Introduction to Deep Learning with Keras\n\n    Image Processing in Python\n\n    Artificial Intelligence (AI) Concepts in Python\n\n    Introduction to Web Scraping in R\n\n    Understanding machine learning\n\n    Introduction to Tableau\n\n-   Implement F1 Score into the code. It is a popular way to see how well models are performing.\n\n-   Tools used for geocoding were QGIS and R, use python for geocoding to check if it will be better than the two used in the project.\n\n-   Add data relating to accessibility to amenities, that could add valuable information to the analysis.\n\nMore objectives to be considered for Year 2 of this project;\n\n-   Could we use AI to determine which communities could benefit most from our project? If we tell it what characteristics of communities make it cause for concern, it could draw the conclusions for us and determine which communities we would want to reach out to.\n\n-   Create an application that can be used by cities.\n\n-   Leveraging spatial analysis tools, we can gain valuable insights and broaden the analytical capabilities specifically in relation to identifying communities that might benefit.\n\n-   Look more into how to use Geospatial Artificial Intelligence (GeoAI).\n\n-   Create AI model that can identify houses, trees, sidewalks and evaluate roof condition when given an image of a neighborhood.\n\n![](/images/drone_to_GIS.png)\n\n## Data source\n\n-   2000 - 2020 Decennial Census\n-   2021 5-Year American Community Survey\n-   Zillow\n-   Google Street View\n-   Vanguard\n-   Beacon\n-   Additional sources can be found [here](https://lehd.ces.census.gov/data/){target=\"_blank\"} and [here](https://data.iowa.gov/Local-Government-Finance/Taxable-Property-Values-in-Iowa-by-Tax-District-an/ig9g-pba5){target=\"_blank\"}.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}